{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import uuid\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dump the data news_snetiment into gcs bucket\n",
    "\n",
    "develop modular way dump the pull into gcp -> see alphaverseScrape.ipynb\n",
    "\n",
    "change bucket to us iowa to match with rafas mongo db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "#update filepath and credentials\n",
    "#####################\n",
    "# set the environment variable directly in the script\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"file path to credentials need to grant roles\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: alphaverse2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GCS Bucket name and initialize storage client\n",
    "\n",
    "#change bucket name\n",
    "####################\n",
    "bucket_name = \"alphaverse2020\"\n",
    "####################\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# check if the bucket is accessible\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    print(f\"Successfully connected to bucket: {bucket_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing bucket: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded news_sentiment_data_20250220_115715.json to GCS\n",
      "File path: gs://alphaverse2020/news_sentiment/news_sentiment_data_20250220_115715.json\n"
     ]
    }
   ],
   "source": [
    "# dump local data /Users/catalinabartholomew/Documents/msdsMac/springMod1/DistDataSys/project/sentiment/news_sentiment_data_20250220_115715.json\n",
    "# into gcp bucket\n",
    "\n",
    "\n",
    "def upload_local_json_to_gcs(file_path, bucket_name, destination_prefix=\"news_sentiment\"):\n",
    "    \"\"\"Uploads a local JSON file to GCS.\"\"\"\n",
    "    try:\n",
    "\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # initialize the GCS client and bucket\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # create blob with destination path\n",
    "        destination_blob_name = f\"{destination_prefix}/{file_name}\"\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        \n",
    "        # upload the file\n",
    "        blob.upload_from_filename(file_path)\n",
    "        \n",
    "        print(f\"Successfully uploaded {file_name} to GCS\")\n",
    "        print(f\"File path: gs://{bucket_name}/{destination_blob_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to GCS: {e}\")\n",
    "        raise\n",
    "\n",
    "# upload the specific JSON file\n",
    "file_path = \"/Users/catalinabartholomew/Documents/msdsMac/springMod1/DistDataSys/project/sentiment/news_sentiment_data_20250220_115715.json\"\n",
    "upload_local_json_to_gcs(file_path, bucket_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def upload_to_gcs(data, bucket_name, prefix=\"news_sentiment\"):\n",
    "#     \"\"\"Uploads JSON data to GCS with a unique timestamped filename.\"\"\"\n",
    "#     try:\n",
    "#         timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")  # Fixed datetime reference\n",
    "#         unique_id = uuid.uuid4().hex[:6]\n",
    "#         file_name = f\"{prefix}_{timestamp}_{unique_id}.json\"\n",
    "\n",
    "#         json_data = json.dumps(data, indent=2)\n",
    "        \n",
    "#         bucket = storage_client.bucket(bucket_name)\n",
    "#         blob = bucket.blob(f\"news_sentiment/{file_name}\")\n",
    "#         blob.upload_from_string(json_data, content_type=\"application/json\")\n",
    "\n",
    "#         # verify upload\n",
    "#         if blob.exists():\n",
    "#             print(f\"Successfully uploaded {file_name} with {len(data)} articles to GCS\")\n",
    "#             print(f\"File path: gs://{bucket_name}/news_sentiment/{file_name}\")\n",
    "#         else:\n",
    "#             print(f\"Upload may have failed for {file_name}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error uploading to GCS: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# mag7_tickers = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\"]\n",
    "\n",
    "# # api_key = \"7EX67VQNOOXOBWKE\"\n",
    "# api_key = \"ZU0OJF3J90D08G85\"\n",
    "# base_url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "\n",
    "# all_articles = []\n",
    "\n",
    "# # loop through tickers to get news articles, or statements\n",
    "# for ticker in mag7_tickers:\n",
    "#     params = {\n",
    "#         \"function\": \"NEWS_SENTIMENT\",\n",
    "#         \"tickers\": ticker,  # query one ticker at a time\n",
    "#         \"time_from\": \"20200101T0000\",  \n",
    "#         \"limit\": 1000,  \n",
    "#         \"sort\": \"LATEST\",\n",
    "#         \"apikey\": api_key\n",
    "#     }\n",
    "\n",
    "#     response = requests.get(base_url, params=params)\n",
    "#     data = response.json()\n",
    "\n",
    "#     # check if the response contains 'feed' key\n",
    "#     if \"feed\" in data and data[\"feed\"]:\n",
    "#         all_articles.extend(data[\"feed\"])\n",
    "#     else:\n",
    "#         print(f\"No articles found for ticker: {ticker}\")\n",
    "\n",
    "# # convert to DataFrame\n",
    "# df = pd.DataFrame(all_articles)\n",
    "\n",
    "# # drop duplicates based on the URL\n",
    "# df.drop_duplicates(subset=[\"url\"], inplace=True)\n",
    "\n",
    "# # debugging : check if articles are found and print the number\n",
    "# print(f\"Total articles found: {len(all_articles)}\")\n",
    "# print(f\"Unique articles after deduplication: {len(df)}\")\n",
    "\n",
    "# # if articles are found, upload to GCS\n",
    "# if not df.empty:\n",
    "#     batch_size = 500\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         batch = df.iloc[i:i + batch_size].to_dict(orient=\"records\")\n",
    "#         print(f\"Uploading batch {i // batch_size + 1} with {len(batch)} articles...\")\n",
    "#         upload_to_gcs(batch, bucket_name)\n",
    "# else:\n",
    "#     print(\"No articles to upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files in bucket:\n",
      "- news_sentiment/news_sentiment_data_20250220_115715.json\n"
     ]
    }
   ],
   "source": [
    "#check bucket contents\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blobs = bucket.list_blobs(prefix=\"news_sentiment/\")\n",
    "print(\"\\nFiles in bucket:\")\n",
    "for blob in blobs:\n",
    "    print(f\"- {blob.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DistDataSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
